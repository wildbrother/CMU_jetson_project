{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action recognition using TAO ActionRecognitionNet\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Train 3D RGB only for action recognition on the subset of [HMDB51](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/) dataset.\n",
    "* Evaluate the trained model.\n",
    "* Run Inference on the trained model.\n",
    "* Export the trained model to a .etlt file for deployment to DeepStream.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of ActionRecognitionNet using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate trained models](#head-5)\n",
    "6. [Inferences](#head-6)\n",
    "7. [Deploy](#head-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HOST_DATA_DIR=/home/sandia/tao/data\n",
      "env: HOST_SPECS_DIR=/home/sandia/tao/specs\n",
      "env: HOST_RESULTS_DIR=/home/sandia/tao/results\n",
      "env: KEY=nvidia_tao\n"
     ]
    }
   ],
   "source": [
    "%env HOST_DATA_DIR=/home/sandia/tao/data\n",
    "# note: You could set the HOST_SPECS_DIR to folder of the experiments specs downloaded with the notebook\n",
    "%env HOST_SPECS_DIR=/home/sandia/tao/specs\n",
    "%env HOST_RESULTS_DIR=/home/sandia/tao/results\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "%env KEY = nvidia_tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tlt_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tlt_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/sandia/tao/data\",\n",
      "            \"destination\": \"/data\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/sandia/tao/specs\",\n",
      "            \"destination\": \"/specs\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/sandia/tao/results\",\n",
      "            \"destination\": \"/results\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/sandia/.cache\",\n",
      "            \"destination\": \"/root/.cache\"\n",
      "        }\n",
      "    ],\n",
      "    \"DockerOptions\": {\n",
      "        \"shm_size\": \"16G\",\n",
      "        \"ulimits\": {\n",
      "            \"memlock\": -1,\n",
      "            \"stack\": 67108864\n",
      "        }\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.6.9 < 3.8.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1 (mine: 3.9.0)\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8416 sha256=b1ff09957e7a314e5b876fb53cbb2706e76cfce6102151ec27ca309d58037ef7\n",
      "  Stored in directory: /home/sandia/.cache/pip/wheels/1a/79/65/9cb980b5f481843cd9896e1579abc1c1f608b5f9e60ca90e03\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvidia-tao\n",
      "  Downloading nvidia_tao-0.1.21-py3-none-any.whl (149 kB)\n",
      "     |████████████████████████████████| 149 kB 8.1 MB/s            \n",
      "\u001b[?25hCollecting tabulate==0.8.7\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting urllib3==1.25.10\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "     |████████████████████████████████| 127 kB 97.2 MB/s            \n",
      "\u001b[?25hCollecting docker==4.3.1\n",
      "  Downloading docker-4.3.1-py2.py3-none-any.whl (145 kB)\n",
      "     |████████████████████████████████| 145 kB 88.5 MB/s            \n",
      "\u001b[?25hCollecting idna==2.10\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 83.1 MB/s            \n",
      "\u001b[?25hCollecting websocket-client==0.57.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "     |████████████████████████████████| 200 kB 86.9 MB/s            \n",
      "\u001b[?25hCollecting six==1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docker-pycreds==0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting certifi==2020.6.20\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 97.7 MB/s            \n",
      "\u001b[?25hCollecting requests==2.24.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 34.9 MB/s             \n",
      "\u001b[?25hCollecting chardet==3.0.4\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     |████████████████████████████████| 133 kB 94.7 MB/s            \n",
      "\u001b[?25hInstalling collected packages: urllib3, six, idna, chardet, certifi, websocket-client, requests, tabulate, docker-pycreds, docker, nvidia-tao\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 docker-4.3.1 docker-pycreds-0.4.0 idna-2.10 nvidia-tao-0.1.21 requests-2.24.0 six-1.15.0 tabulate-0.8.7 urllib3-1.25.10 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: tao: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the [HMDB51](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/) dataset for the tutorial. Download the HMDB51 dataset and unrar them firstly (We choose fall_floor/ride_bike for this tutorial): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for sandia: \n",
      "[sudo] password for sandia: \n"
     ]
    }
   ],
   "source": [
    "# install unrar\n",
    "# NOTE: The following commands require `sudo`. You can run the command outside the notebook.\n",
    "!sudo apt update\n",
    "!sudo apt-get install unrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for sandia: \n",
      "mkdir: cannot create directory ‘/videos’: Permission denied\n",
      "mkdir: cannot create directory ‘/raw_data’: Permission denied\n",
      "\n",
      "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
      "\n",
      "Cannot open /videos/fall_floor.rar\n",
      "No such file or directory\n",
      "No files to extract\n",
      "\n",
      "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
      "\n",
      "Cannot open /videos/ride_bike.rar\n",
      "No such file or directory\n",
      "No files to extract\n"
     ]
    }
   ],
   "source": [
    "# download the dataset and unrar the files\n",
    "!wget -P $HOST_DATA_DIR http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
    "!mkdir -p $HOST_DATA_DIR/videos && unrar x $HOST_DATA_DIR/hmdb51_org.rar $HOST_DATA_DIR/videos\n",
    "!mkdir -p $HOST_DATA_DIR/raw_data\n",
    "!unrar x $HOST_DATA_DIR/videos/fall_floor.rar $HOST_DATA_DIR/raw_data\n",
    "!unrar x $HOST_DATA_DIR/videos/ride_bike.rar $HOST_DATA_DIR/raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the dataset process script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA-AI-IOT/tao_toolkit_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependency for data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install xmltodict opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the process script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandia/tao\n",
      "/home/sandia/tao\r\n"
     ]
    }
   ],
   "source": [
    "%cd ../../\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandia/tao/data/raw_data\n",
      "/home/sandia/tao/data/processed_data\n",
      "Preprocess pullup\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 71.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 100.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 88.0\n",
      "f cnt: 88.0\n",
      "f cnt: 88.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 84.0\n",
      "f cnt: 84.0\n",
      "f cnt: 84.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 56.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 68.0\n",
      "f cnt: 114.0\n",
      "f cnt: 84.0\n",
      "f cnt: 82.0\n",
      "f cnt: 80.0\n",
      "f cnt: 43.0\n",
      "f cnt: 80.0\n",
      "f cnt: 84.0\n",
      "f cnt: 106.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 88.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "Preprocess pushup\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 92.0\n",
      "f cnt: 114.0\n",
      "f cnt: 90.0\n",
      "f cnt: 108.0\n",
      "f cnt: 102.0\n",
      "f cnt: 112.0\n",
      "f cnt: 134.0\n",
      "f cnt: 144.0\n",
      "f cnt: 229.0\n",
      "f cnt: 231.0\n",
      "f cnt: 147.0\n",
      "f cnt: 154.0\n",
      "f cnt: 144.0\n",
      "f cnt: 166.0\n",
      "f cnt: 106.0\n",
      "f cnt: 85.0\n",
      "f cnt: 80.0\n",
      "f cnt: 74.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 68.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 82.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 105.0\n",
      "f cnt: 87.0\n",
      "f cnt: 93.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 71.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 79.0\n",
      "f cnt: 84.0\n",
      "f cnt: 78.0\n",
      "f cnt: 78.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 81.0\n",
      "f cnt: 80.0\n",
      "f cnt: 81.0\n",
      "Preprocess situp\n",
      "f cnt: 99.0\n",
      "f cnt: 109.0\n",
      "f cnt: 103.0\n",
      "f cnt: 106.0\n",
      "f cnt: 102.0\n",
      "f cnt: 111.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 68.0\n",
      "f cnt: 80.0\n",
      "f cnt: 106.0\n",
      "f cnt: 101.0\n",
      "f cnt: 103.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 58.0\n",
      "f cnt: 80.0\n",
      "f cnt: 139.0\n",
      "f cnt: 84.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 190.0\n",
      "f cnt: 131.0\n",
      "f cnt: 94.0\n",
      "f cnt: 100.0\n",
      "f cnt: 110.0\n",
      "f cnt: 128.0\n",
      "f cnt: 111.0\n",
      "f cnt: 142.0\n",
      "f cnt: 167.0\n",
      "f cnt: 163.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 95.0\n",
      "f cnt: 132.0\n",
      "f cnt: 94.0\n",
      "f cnt: 77.0\n",
      "f cnt: 84.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 79.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 130.0\n",
      "f cnt: 122.0\n",
      "f cnt: 114.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 82.0\n",
      "f cnt: 114.0\n",
      "f cnt: 98.0\n",
      "f cnt: 105.0\n",
      "f cnt: 106.0\n",
      "f cnt: 113.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 121.0\n",
      "f cnt: 98.0\n",
      "f cnt: 102.0\n",
      "f cnt: 95.0\n",
      "f cnt: 97.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 93.0\n",
      "f cnt: 110.0\n",
      "f cnt: 105.0\n",
      "f cnt: 95.0\n",
      "f cnt: 90.0\n",
      "f cnt: 95.0\n",
      "f cnt: 80.0\n",
      "f cnt: 85.0\n",
      "f cnt: 84.0\n",
      "f cnt: 88.0\n",
      "f cnt: 92.0\n",
      "f cnt: 83.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 83.0\n",
      "f cnt: 110.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 129.0\n",
      "f cnt: 118.0\n",
      "f cnt: 120.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n",
      "f cnt: 80.0\n"
     ]
    }
   ],
   "source": [
    "!cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && bash ./preprocess_HMDB_RGB.sh $HOST_DATA_DIR/raw_data $HOST_DATA_DIR/processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-01 03:50:54--  http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar\n",
      "Resolving serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)... 128.148.254.114\n",
      "Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar [following]\n",
      "--2022-06-01 03:50:55--  https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar\n",
      "Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 199521 (195K)\n",
      "Saving to: ‘/home/sandia/tao/data/test_train_splits.rar’\n",
      "\n",
      "test_train_splits.r 100%[===================>] 194.84K   190KB/s    in 1.0s    \n",
      "\n",
      "2022-06-01 03:50:57 (190 KB/s) - ‘/home/sandia/tao/data/test_train_splits.rar’ saved [199521/199521]\n",
      "\n",
      "\n",
      "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from /home/sandia/tao/data/test_train_splits.rar\n",
      "\n",
      "Creating    /home/sandia/tao/data/splits/testTrainMulti_7030_splits   OK\n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/brush_hair_test_split1.txt     0  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/brush_hair_test_split2.txt     1  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/brush_hair_test_split3.txt     1  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/cartwheel_test_split1.txt     2  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/cartwheel_test_split2.txt     2  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/cartwheel_test_split3.txt     3  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/catch_test_split1.txt     4  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/catch_test_split2.txt     4  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/catch_test_split3.txt     5  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/chew_test_split1.txt     5  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/chew_test_split2.txt     6  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/chew_test_split3.txt     6  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/clap_test_split1.txt     7  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/clap_test_split2.txt     7  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/clap_test_split3.txt     8  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/climb_stairs_test_split1.txt     9  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/climb_stairs_test_split2.txt    10  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/climb_stairs_test_split3.txt    10  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/climb_test_split1.txt    11  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/climb_test_split2.txt    12  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/climb_test_split3.txt    12  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/dive_test_split1.txt    13  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/dive_test_split2.txt    14  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/dive_test_split3.txt    14  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/draw_sword_test_split1.txt    15  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/draw_sword_test_split2.txt    15  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/draw_sword_test_split3.txt    16  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/dribble_test_split1.txt    17  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/dribble_test_split2.txt    17  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/dribble_test_split3.txt    18  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/drink_test_split1.txt    19  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/drink_test_split2.txt    20  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/drink_test_split3.txt    20  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/eat_test_split1.txt    21  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/eat_test_split2.txt    21  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/eat_test_split3.txt    22  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/fall_floor_test_split1.txt    23  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/fall_floor_test_split2.txt    23  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/fall_floor_test_split3.txt    24  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/fencing_test_split1.txt    25  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/fencing_test_split2.txt    25  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/fencing_test_split3.txt    26  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/flic_flac_test_split1.txt    26  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/flic_flac_test_split2.txt    27  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/flic_flac_test_split3.txt    27  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/golf_test_split1.txt    28  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/golf_test_split2.txt    29  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/golf_test_split3.txt    29  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/handstand_test_split1.txt    30  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/handstand_test_split2.txt    31  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/handstand_test_split3.txt    32  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/hit_test_split1.txt    32  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/hit_test_split2.txt    33  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/hit_test_split3.txt    34  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/hug_test_split1.txt    34  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/hug_test_split2.txt    35  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/hug_test_split3.txt    35  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/jump_test_split1.txt    36  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/jump_test_split2.txt    37  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/jump_test_split3.txt    37  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kick_ball_test_split1.txt    38  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kick_ball_test_split2.txt    39  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kick_ball_test_split3.txt    40  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kick_test_split1.txt    40  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kick_test_split2.txt    41  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kick_test_split3.txt    41  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kiss_test_split1.txt    42  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kiss_test_split2.txt    42  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/kiss_test_split3.txt    42  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/laugh_test_split1.txt    43  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/laugh_test_split2.txt    43  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/laugh_test_split3.txt    44  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pick_test_split1.txt    45  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pick_test_split2.txt    45  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pick_test_split3.txt    46  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pour_test_split1.txt    47  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pour_test_split2.txt    48  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pour_test_split3.txt    48  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pullup_test_split1.txt    49  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pullup_test_split2.txt    49  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pullup_test_split3.txt    50  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/punch_test_split1.txt    50  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/punch_test_split2.txt    51  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/punch_test_split3.txt    52  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pushup_test_split1.txt    52  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pushup_test_split2.txt    53  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/pushup_test_split3.txt    53  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/push_test_split1.txt    54  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/push_test_split2.txt    54  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/push_test_split3.txt    55  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/ride_bike_test_split1.txt    55  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/ride_bike_test_split2.txt    56  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/ride_bike_test_split3.txt    56  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/ride_horse_test_split1.txt    56  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/ride_horse_test_split2.txt    57  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/ride_horse_test_split3.txt    57  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/run_test_split1.txt    58  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/run_test_split2.txt    59  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/run_test_split3.txt    60  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shake_hands_test_split1.txt    62  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shake_hands_test_split2.txt    63  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shake_hands_test_split3.txt    64  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_ball_test_split1.txt    64  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_ball_test_split2.txt    65  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_ball_test_split3.txt    65  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_bow_test_split1.txt    66  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_bow_test_split2.txt    66  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_bow_test_split3.txt    67  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_gun_test_split1.txt    67  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_gun_test_split2.txt    67  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/shoot_gun_test_split3.txt    68  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/situp_test_split1.txt    69  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/situp_test_split2.txt    69  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/situp_test_split3.txt    70  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sit_test_split1.txt    71  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sit_test_split2.txt    71  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sit_test_split3.txt    72  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/smile_test_split1.txt    73  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/smile_test_split2.txt    73  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/smile_test_split3.txt    73  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/smoke_test_split1.txt    74  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/smoke_test_split2.txt    74  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/smoke_test_split3.txt    75  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/somersault_test_split1.txt    76  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/somersault_test_split2.txt    76  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/somersault_test_split3.txt    77  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/stand_test_split1.txt    78  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/stand_test_split2.txt    79  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/stand_test_split3.txt    80  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/swing_baseball_test_split1.txt    80  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/swing_baseball_test_split2.txt    81  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/swing_baseball_test_split3.txt    81  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sword_exercise_test_split1.txt    82  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sword_exercise_test_split2.txt    83  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sword_exercise_test_split3.txt    83  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sword_test_split1.txt    84  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sword_test_split2.txt    85  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/sword_test_split3.txt    86  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/talk_test_split1.txt    86  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/talk_test_split2.txt    87  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/talk_test_split3.txt    87  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/throw_test_split1.txt    88  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/throw_test_split2.txt    88  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/throw_test_split3.txt    89  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/turn_test_split1.txt    90  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/turn_test_split2.txt    91  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/turn_test_split3.txt    92  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/walk_test_split1.txt    94  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/walk_test_split2.txt    95  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/walk_test_split3.txt    97  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/wave_test_split1.txt    98  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/wave_test_split2.txt    99  OK \n",
      "Extracting  /home/sandia/tao/data/splits/testTrainMulti_7030_splits/wave_test_split3.txt    99  OK \n",
      "All OK\n"
     ]
    }
   ],
   "source": [
    "# download the split files and unrar\n",
    "!wget -P $HOST_DATA_DIR http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar\n",
    "!mkdir -p $HOST_DATA_DIR/splits && unrar x $HOST_DATA_DIR/test_train_splits.rar $HOST_DATA_DIR/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./split_dataset.py\", line 31, in <module>\r\n",
      "    os.makedirs(target_train_path)\r\n",
      "  File \"/usr/lib/python3.6/os.py\", line 220, in makedirs\r\n",
      "    mkdir(name, mode)\r\n",
      "PermissionError: [Errno 13] Permission denied: '/train'\r\n"
     ]
    }
   ],
   "source": [
    "# run split_HMDB to generate training split\n",
    "!cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && python3 ./split_dataset.py $HOST_DATA_DIR/processed_data $HOST_DATA_DIR/splits/testTrainMulti_7030_splits $HOST_DATA_DIR/train  $HOST_DATA_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxrwxr-x 72 sandia sandia 12288  6월  1 03:51 pullup\n",
      "drwxrwxr-x 72 sandia sandia 12288  6월  1 03:51 pushup\n",
      "drwxrwxr-x 72 sandia sandia 12288  6월  1 03:51 situp\n",
      "ls: cannot access '/home/sandia/tao/data/train/ride_bike': No such file or directory\n",
      "total 12\n",
      "drwxrwxr-x 32 sandia sandia 4096  6월  1 03:51 pullup\n",
      "drwxrwxr-x 32 sandia sandia 4096  6월  1 03:51 pushup\n",
      "drwxrwxr-x 32 sandia sandia 4096  6월  1 03:51 situp\n",
      "ls: cannot access '/home/sandia/tao/data/test/ride_bike': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "!ls -l $HOST_DATA_DIR/train\n",
    "!ls -l $HOST_DATA_DIR/train/ride_bike\n",
    "!ls -l $HOST_DATA_DIR/test\n",
    "!ls -l $HOST_DATA_DIR/test/ride_bike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide scripts to preprocess [SHAD](https://best.sjtu.edu.cn/Data/View/990) dataset. The following cells for processing SHAD dataset is `Optional`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OPTIONAL:` Download the app based on NVOF SDK to generate optical flow. It is packaged with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!echo <passwd> | sudo -S apt install -y libfreeimage-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create train and test dir for raw videos and labels\n",
    "# !mkdir -p $HOST_DATA_DIR/train_raw && mkdir -p $HOST_DATA_DIR/test_raw\n",
    "# # download the dataset and unrar the files\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Bend-train.rar\n",
    "# !unrar x $HOST_DATA_DIR/Bend-train.rar $HOST_DATA_DIR/train_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Bend-test.rar\n",
    "# !unrar x $HOST_DATA_DIR/Bend-test.rar $HOST_DATA_DIR/test_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Fall-train.rar\n",
    "# !unrar x $HOST_DATA_DIR/Fall-train.rar $HOST_DATA_DIR/train_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Fall-test.rar\n",
    "# !unrar x $HOST_DATA_DIR/Fall-test.rar $HOST_DATA_DIR/test_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Squa-train.rar\n",
    "# !unrar x $HOST_DATA_DIR/Squa-train.rar $HOST_DATA_DIR/train_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Squa-test.rar\n",
    "# !unrar x $HOST_DATA_DIR/Squa-test.rar $HOST_DATA_DIR/test_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Sits-train.rar\n",
    "# !unrar x $HOST_DATA_DIR/Sits-train.rar $HOST_DATA_DIR/train_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Sits-test.rar\n",
    "# !unrar x $HOST_DATA_DIR/Sits-test.rar $HOST_DATA_DIR/test_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Walk-train.rar\n",
    "# !unrar x $HOST_DATA_DIR/Walk-train.rar $HOST_DATA_DIR/train_raw\n",
    "# !wget -P $HOST_DATA_DIR https://best.sjtu.edu.cn/Assets/userfiles/sys_eb538c1c-65ff-4e82-8e6a-a1ef01127fed/files/ZIP/Walk-test.rar\n",
    "# !unrar x $HOST_DATA_DIR/Walk-test.rar $HOST_DATA_DIR/test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OPTIONAL` Run the process script for SHAD. \n",
    "\n",
    "`IMPORTANT NOTE`: to run the `process_SHAD.sh` generating optical flow, a Turing or Ampere above GPU is needed. You could run with `preprocess_SHAD_RGB.sh` to play with RGB frames only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && ./preprocess_SHAD.sh $HOST_DATA_DIR/train_raw $HOST_DATA_DIR/train\n",
    "# ! cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && ./preprocess_SHAD_RGB.sh $HOST_DATA_DIR/train_raw $HOST_DATA_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && ./preprocess_SHAD.sh $HOST_DATA_DIR/test_raw $HOST_DATA_DIR/test\n",
    "# ! cd tao_toolkit_recipes/tao_action_recognition/data_generation/ && ./preprocess_SHAD_RGB.sh $HOST_DATA_DIR/test_raw $HOST_DATA_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # verify\n",
    "# !ls -l $HOST_DATA_DIR/train\n",
    "# !ls -l $HOST_DATA_DIR/train/bend\n",
    "# !ls -l $HOST_DATA_DIR/test\n",
    "# !ls -l $HOST_DATA_DIR/test/bend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download pretrained model from NGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to https://ngc.nvidia.com and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_cat_linux.zip\n",
      "--2022-06-01 03:52:56--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 13.225.121.94, 13.225.121.41, 13.225.121.127, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|13.225.121.94|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32617431 (31M) [application/zip]\n",
      "Saving to: ‘/home/sandia/tao/results/ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  31.11M  14.6MB/s    in 2.1s    \n",
      "\n",
      "2022-06-01 03:52:59 (14.6 MB/s) - ‘/home/sandia/tao/results/ngccli/ngccli_cat_linux.zip’ saved [32617431/32617431]\n",
      "\n",
      "Archive:  /home/sandia/tao/results/ngccli/ngccli_cat_linux.zip\n",
      "  inflating: /home/sandia/tao/results/ngccli/ngc  \n",
      " extracting: /home/sandia/tao/results/ngccli/ngc.md5  \n"
     ]
    }
   ],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "import os\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $HOST_RESULTS_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $HOST_RESULTS_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $HOST_RESULTS_DIR/ngccli\n",
    "!unzip -u \"$HOST_RESULTS_DIR/ngccli/$CLI\" -d $HOST_RESULTS_DIR/ngccli/\n",
    "!rm $HOST_RESULTS_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli:{}\".format(os.getenv(\"HOST_RESULTS_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandia/tao/results/ngccli\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| Versi | Accur | Epoch | Batch | GPU   | Memor | File  | Statu | Creat |\n",
      "| on    | acy   | s     | Size  | Model | y Foo | Size  | s     | ed    |\n",
      "|       |       |       |       |       | tprin |       |       | Date  |\n",
      "|       |       |       |       |       | t     |       |       |       |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| train | 88.0  | 120   | 1     | V100  | 850.1 | 850.1 | UPLOA | May   |\n",
      "| able_ |       |       |       |       |       | 1 MB  | D_COM | 17,   |\n",
      "| v2.0  |       |       |       |       |       |       | PLETE | 2022  |\n",
      "| train | 88.0  | 120   | 1     | V100  | 426.2 | 426.1 | UPLOA | Nov   |\n",
      "| able_ |       |       |       |       |       | 6 MB  | D_COM | 23,   |\n",
      "| v1.0  |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| deplo | 88.0  | 120   | 1     | V100  | 339.8 | 339.7 | UPLOA | May   |\n",
      "| yable |       |       |       |       |       | 8 MB  | D_COM | 17,   |\n",
      "| _v2.0 |       |       |       |       |       |       | PLETE | 2022  |\n",
      "| deplo | 90.0  | 120   | 1     | V100  | 170.3 | 170.3 | UPLOA | Oct   |\n",
      "| yable |       |       |       |       |       | 3 MB  | D_COM | 22,   |\n",
      "| _v1.0 |       |       |       |       |       |       | PLETE | 2021  |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "%cd results/ngccli\n",
    "!./ngc registry model list nvidia/tao/actionrecognitionnet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $HOST_RESULTS_DIR/pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 363.87 MB in 1m 19s, Download speed: 4.6 MB/s                \n",
      "----------------------------------------------------\n",
      "Transfer id: actionrecognitionnet_vtrainable_v1.0 Download status: Completed.\n",
      "Downloaded local path: /home/sandia/tao/results/pretrained/actionrecognitionnet_vtrainable_v1.0\n",
      "Total files downloaded: 2 \n",
      "Total downloaded size: 363.87 MB\n",
      "Started at: 2022-06-01 03:55:53.952336\n",
      "Completed at: 2022-06-01 03:57:13.067109\n",
      "Duration taken: 1m 19s\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pull pretrained model from NGC \n",
    "!ngc registry model download-version \"nvidia/tao/actionrecognitionnet:trainable_v1.0\" --dest $HOST_RESULTS_DIR/pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 436400\r\n",
      "-rw------- 1 sandia sandia 114846245  6월  1 03:57 resnet18_2d_rgb_hmdb5_32.tlt\r\n",
      "-rw------- 1 sandia sandia 332019621  6월  1 03:57 resnet18_3d_rgb_hmdb5_32.tlt\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $HOST_RESULTS_DIR/pretrained/actionrecognitionnet_vtrainable_v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* model_config: configure the model setting\n",
    "    * model_type: type of model, rgb/of/joint\n",
    "    * backbone: resnet18/34/50/101/152 \n",
    "    * rgb_seq_length: length of RGB input sequence\n",
    "    * input_type: 2d/3d\n",
    "    * sample_strategy: consecutive\n",
    "    * dropout_ratio: probability to drop the hidden units\n",
    "* train_config: configure the training hyperparameters\n",
    "    * optim_config\n",
    "    * epochs\n",
    "    * checkpoint_interval\n",
    "* dataset_config: configure the dataset and augmentation methods\n",
    "    * train_dataset_dir\n",
    "    * val_dataset_dir\n",
    "    * label_map: map the class label to id\n",
    "    * output_shape\n",
    "    * batch_size\n",
    "    * workers: number of workers to do data loading\n",
    "    * clips_per_video: number of clips to be sampled from single video\n",
    "    * augmentation_config\n",
    "\n",
    "Please refer to the TAO documentation about ActionRecognitionNet to get all the parameters that are configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: /results/rgb_3d_ptm\r\n",
      "encryption_key: nvidia_tao\r\n",
      "\r\n",
      "## Model Configuration\r\n",
      "model_config:                                                                        \r\n",
      "   model_type: rgb                                                                     \r\n",
      "   input_type: \"3d\"                                                                     \r\n",
      "   backbone: resnet18                                                                 \r\n",
      "   rgb_seq_length: 32  ## Change from 3 to 32 frame sequence                                                                \r\n",
      "   rgb_pretrained_num_classes: 5                                                   \r\n",
      "   sample_strategy: consecutive                                                        \r\n",
      "   sample_rate: 1\r\n",
      "   dropout_ratio: 0.0\r\n",
      "\r\n",
      "# Training Hyperparameter configuration\r\n",
      "train_config: \r\n",
      "   optim:   \r\n",
      "      lr: 0.001  \r\n",
      "      momentum: 0.9   \r\n",
      "      weight_decay: 0.0001   \r\n",
      "      lr_scheduler: MultiStep   \r\n",
      "      lr_steps: [5, 15, 25]   \r\n",
      "      lr_decay: 0.1 \r\n",
      "   epochs: 20  ## Number of Epochs to train\r\n",
      "   checkpoint_interval: 1  ## Saves model checkpoint interval\r\n",
      "\r\n",
      "## Dataset configuration\r\n",
      "dataset_config: \r\n",
      "   train_dataset_dir: /data/train  ## Modify to use your train dataset\r\n",
      "   val_dataset_dir: /data/test     ## Modify to use your test dataset\r\n",
      "   ## Label maps for new classes. Modify this for your custom classes\r\n",
      "   label_map:   \r\n",
      "      # pushup: 0   \r\n",
      "      # pullup: 1   \r\n",
      "      # situp: 2   \r\n",
      "      brush_hair: 0\r\n",
      "      cartwheel: 1\r\n",
      "      catch: 2\r\n",
      "      chew: 3\r\n",
      "      clap: 4\r\n",
      "      climb: 5\r\n",
      "      climb_stairs: 6\r\n",
      "      dive: 7\r\n",
      "      draw_sword: 8\r\n",
      "      dribble: 9\r\n",
      "      drink: 10\r\n",
      "      eat: 11\r\n",
      "      fall_floor: 12\r\n",
      "      fencing: 13\r\n",
      "      flic_flac: 14\r\n",
      "      golf: 15\r\n",
      "      handstand: 16\r\n",
      "      hit: 17\r\n",
      "      hug: 18\r\n",
      "      jump: 19\r\n",
      "      kick: 20\r\n",
      "      kick_ball: 21\r\n",
      "      kiss: 22\r\n",
      "      laugh: 23\r\n",
      "      pick: 24\r\n",
      "      pour: 25\r\n",
      "      pullup: 26\r\n",
      "      punch: 27\r\n",
      "      push: 28\r\n",
      "      pushup: 29\r\n",
      "      ride_bike: 30\r\n",
      "      ride_horse: 31\r\n",
      "      run: 32\r\n",
      "      shake_hands: 33\r\n",
      "      shoot_ball: 34\r\n",
      "      shoot_bow: 35\r\n",
      "      shoot_fun: 36 \r\n",
      "      sit: 37\r\n",
      "      situp: 38\r\n",
      "      smile: 39\r\n",
      "      smoke: 40\r\n",
      "      somersault: 41\r\n",
      "      stand: 42\r\n",
      "      swing_baseball: 43\r\n",
      "      sword: 44\r\n",
      "      sword_exercise: 45\r\n",
      "      talk: 46\r\n",
      "      throw: 47\r\n",
      "      turn: 48\r\n",
      "      walk: 49\r\n",
      "      wave: 50\r\n",
      "\r\n",
      "   ## Model input resolution\r\n",
      "   output_shape: \r\n",
      "   - 224 \r\n",
      "   - 224 \r\n",
      "   batch_size: 32 \r\n",
      "   workers: 8 \r\n",
      "   clips_per_video: 5\r\n",
      "   augmentation_config:\r\n",
      "      train_crop_type: no_crop\r\n",
      "      horizontal_flip_prob: 0.5\r\n",
      "      rgb_input_mean: [0.5]\r\n",
      "      rgb_input_std: [0.5]\r\n",
      "      val_center_crop: False   \r\n"
     ]
    }
   ],
   "source": [
    "!cat $HOST_SPECS_DIR/train_rgb_3d_finetune.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=/data\n",
      "env: SPECS_DIR=/specs\n",
      "env: RESULTS_DIR=/results\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR = /data\n",
    "%env SPECS_DIR = /specs\n",
    "%env RESULTS_DIR = /results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train 3D model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide pretrained RGB-only model trained on HMDB5 dataset. With the pretrained model, we can even get better accuracy with less epochs.\n",
    "\n",
    "`KNOWN ISSUE`: \n",
    "- 1) The training log will be corrupted by pytorch warning in the notebook:\n",
    "\n",
    "     `[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)` \n",
    "     \n",
    "     To see the full log in std out, please run the command in terminal. \n",
    "- 2) \"=\" in the checkpoint file name should removed before using the checkpoint in command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RGB only model with PTM\n",
      "/bin/bash: tao: command not found\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Train RGB only model with PTM\")\n",
    "!tao action_recognition train \\\n",
    "                  -e $SPECS_DIR/train_rgb_3d_finetune.yaml \\\n",
    "                  -r $RESULTS_DIR/rgb_3d_ptm/2nd \\\n",
    "                  -k $KEY \\\n",
    "                  model_config.rgb_pretrained_model_path=$RESULTS_DIR/pretrained/actionrecognitionnet_vtrainable_v1.0/resnet18_3d_rgb_hmdb5_32.tlt  \\\n",
    "                  model_config.rgb_pretrained_num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted checkpoints:\n",
      "---------------------\n",
      "total 6.2G\r\n",
      "drwxr-xr-x 3 root root 4.0K  6월  1 04:32  lightning_logs\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:33 'ar_model_epoch=00-val_loss=0.94.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:34 'ar_model_epoch=01-val_loss=0.82.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:35 'ar_model_epoch=02-val_loss=0.77.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:37 'ar_model_epoch=03-val_loss=0.75.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:38 'ar_model_epoch=04-val_loss=0.75.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:39 'ar_model_epoch=05-val_loss=0.73.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:40 'ar_model_epoch=06-val_loss=0.76.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:41 'ar_model_epoch=07-val_loss=0.72.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:42 'ar_model_epoch=08-val_loss=0.73.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:43 'ar_model_epoch=09-val_loss=0.74.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:44 'ar_model_epoch=10-val_loss=0.76.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:45 'ar_model_epoch=11-val_loss=0.73.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:47 'ar_model_epoch=12-val_loss=0.72.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:48 'ar_model_epoch=13-val_loss=0.73.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:49 'ar_model_epoch=14-val_loss=0.73.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:50 'ar_model_epoch=15-val_loss=0.73.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:51 'ar_model_epoch=16-val_loss=0.72.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:52 'ar_model_epoch=17-val_loss=0.74.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:53 'ar_model_epoch=18-val_loss=0.74.tlt'\r\n",
      "-rw-r--r-- 1 root root 317M  6월  1 04:54 'ar_model_epoch=19-val_loss=0.77.tlt'\r\n"
     ]
    }
   ],
   "source": [
    "print('Encrypted checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $HOST_RESULTS_DIR/rgb_3d_ptm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rename a model: Note that the training is not deterministic, so you may change the model name accordingly.\n",
      "---------------------\n",
      "[sudo] password for sandia: \n",
      "ls: cannot access '/home/sandia/tao/results/rgb_3d_ptm/rgb_only_model.tlt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "print('Rename a model: Note that the training is not deterministic, so you may change the model name accordingly.')\n",
    "print('---------------------')\n",
    "# NOTE: The following command may require `sudo`. You can run the command outside the notebook.\n",
    "!mv $HOST_RESULTS_DIR/rgb_3d_ptm/ar_model_epoch=16-val_loss=0.72.tlt $HOST_RESULTS_DIR/rgb_3d_ptm/rgb_only_model.tlt \n",
    "!ls -ltrh $HOST_RESULTS_DIR/rgb_3d_ptm/rgb_only_model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `OPTIONAL` 4.2 Train 2D model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Important Note` The following cells are using SHAD dataset. \n",
    "\n",
    "Firstly, we will train a 2D RGB-only model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Train RGB only model from scratch\")\n",
    "# !tao action_recognition train \\\n",
    "#                   -e $SPECS_DIR/train_rgb_2d.yaml \\\n",
    "#                   -r $RESULTS_DIR/rgb_2d \\\n",
    "#                   -k $KEY \\\n",
    "#                   dataset_config.train_dataset_dir=$DATA_DIR/train \\\n",
    "#                   dataset_config.val_dataset_dir=$DATA_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"To resume training from a checkpoint, set the resume_training_checkpoint_path option to be the .tlt you want to resume from\")\n",
    "# print(\"remember to remove the `=` in the checkpoint's file name\")\n",
    "# !tao action_recognition train \\\n",
    "#                   -e $SPECS_DIR/train_rgb_2d.yaml \\\n",
    "#                   -r $RESULTS_DIR/rgb_2d \\\n",
    "#                   -k $KEY \\\n",
    "#                   resume_training_checkpoint_path="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Encrypted checkpoints:')\n",
    "# print('---------------------')\n",
    "# !ls -ltrh $HOST_RESULTS_DIR/rgb_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Rename a model: ')\n",
    "# print('---------------------')\n",
    "# !echo <passwd> | sudo -S mv $HOST_RESULTS_DIR/rgb_2d/ar_model_epoch=21-val_loss=0.88.tlt $HOST_RESULTS_DIR/rgb_2d/rgb_only_model.tlt \n",
    "# !ls -ltrh $HOST_RESULTS_DIR/rgb_2d/rgb_only_model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train a 2D optical flow only model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train optical flow only model from scratch\")\n",
    "# !tao action_recognition train \\\n",
    "#                   -e $SPECS_DIR/train_of_2d.yaml \\\n",
    "#                   -r $RESULTS_DIR/of_2d \\\n",
    "#                   -k $KEY \\\n",
    "#                   dataset_config.train_dataset_dir=$DATA_DIR/train \\\n",
    "#                   dataset_config.val_dataset_dir=$DATA_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Encrypted checkpoints:')\n",
    "# print('---------------------')\n",
    "# !ls -ltrh $HOST_RESULTS_DIR/of_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Rename a model: ')\n",
    "# print('---------------------')\n",
    "# !echo <passwd> | sudo -S mv $HOST_RESULTS_DIR/of_2d/ar_model_epoch=29-val_loss=0.94.tlt $HOST_RESULTS_DIR/of_2d/of_only_model.tlt \n",
    "# !ls -ltrh $HOST_RESULTS_DIR/of_2d/of_only_model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will train a 2D joint model which consumed both RGB frames and optical flow frames based on two pretrained single stream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train joint model based on RGB and OF model\")\n",
    "# !tao action_recognition train \\\n",
    "#                   -e $SPECS_DIR/train_joint_2d.yaml \\\n",
    "#                   -r $RESULTS_DIR/joint_2d \\\n",
    "#                   -k $KEY \\\n",
    "#                   model_config.rgb_pretrained_model_path=$RESULTS_DIR/rgb_2d/rgb_only_model.tlt \\\n",
    "#                   model_config.of_pretrained_model_path=$RESULTS_DIR/of_2d/of_only_model.tlt \\\n",
    "#                   dataset_config.train_dataset_dir=$DATA_DIR/train \\\n",
    "#                   dataset_config.val_dataset_dir=$DATA_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Encrypted checkpoints:')\n",
    "# print('---------------------')\n",
    "# !ls -ltrh $HOST_RESULTS_DIR/joint_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Rename a model: ')\n",
    "# print('---------------------')\n",
    "# !echo <passwd> | sudo -S mv $HOST_RESULTS_DIR/joint_2d/ar_model_epoch=16-val_loss=0.60.tlt $HOST_RESULTS_DIR/joint_2d/joint_model.tlt \n",
    "# !ls -ltrh $HOST_RESULTS_DIR/joint_2d/joint_model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "We provide two different sample strategy to evaluate the pretrained model on video clips.\n",
    "\n",
    "* `center` mode: pick up the middle frames of a sequence to do inference. For example, if the model requires 32 frames as input and a video clip has 128 frames, then we will choose the frames from index 48 to index 79 to do the inference. \n",
    "* `conv` mode: convolutionly sample 10 sequences out of a single video and do inference. The final results are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate RGB model trained with PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 06:33:36,731 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-01 06:33:36,879 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-01 06:33:36,979 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/sandia/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "/home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/scripts/evaluate.py:154: UserWarning: \n",
      "'evaluate_rgb.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "ResNet3d(\n",
      "  (conv1): Conv3d(3, 64, kernel_size=(5, 7, 7), stride=(2, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=2, padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc_cls): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "/opt/conda/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "100%|███████████████████████████████████████████| 90/90 [00:04<00:00, 18.98it/s]\n",
      "*******************************\n",
      "pushup        33.33\n",
      "pullup        100.0\n",
      "situp         60.0\n",
      "*******************************\n",
      "Total accuracy: 64.444\n",
      "Average class accuracy: 64.444\n",
      "2022-06-01 06:33:53,718 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao action_recognition evaluate \\\n",
    "                    -e $SPECS_DIR/evaluate_rgb.yaml \\\n",
    "                    -k $KEY \\\n",
    "                    model=$RESULTS_DIR/rgb_3d_ptm/rgb_only_model.tlt  \\\n",
    "                    batch_size=1 \\\n",
    "                    test_dataset_dir=$DATA_DIR/test \\\n",
    "                    video_eval_mode=center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inferences <a class=\"anchor\" id=\"head-5\"></a>\n",
    "In this section, we run the action recognition inference tool to generate inferences with the trained RGB models and print the results. \n",
    "\n",
    "There are also two modes for inference just like evaluation: `center` mode and `conv` mode. And the final output will show each input sequence label in the videos like:\n",
    "`[video_sample_path] [labels list for sequences in the video sample]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 06:50:00,072 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-01 06:50:00,214 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-01 06:50:00,309 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/sandia/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-05-31 21:50:06 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/scripts/inference.py:88: UserWarning: \n",
      "    'infer_rgb.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "ResNet3d(\n",
      "  (conv1): Conv3d(3, 64, kernel_size=(5, 7, 7), stride=(2, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=2, padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc_cls): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "[NeMo W 2022-05-31 21:50:08 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "      stream(template_mgs % msg_args)\n",
      "    \n",
      "100%|███████████████████████████████████████████| 30/30 [00:02<00:00, 14.26it/s]\n",
      "/data/test/pullup/32_Pull-ups_at_193IBS_pullup_f_cm_np1_ba_med_1 : ['pullup']\n",
      "/data/test/pullup/FREESTYLIN_ON_THE_PULL-BAR_pullup_u_cm_np1_fr_med_1 : ['pullup']\n",
      "/data/test/pullup/FREESTYLIN_ON_THE_PULL-BAR_pullup_u_cm_np1_le_med_2 : ['pullup']\n",
      "/data/test/pullup/20_Marine_Corps_Pull_Ups_-_JimmyDShea_pullup_u_cm_np1_ba_bad_0 : ['pullup']\n",
      "/data/test/pullup/35_pull_ups_pullup_f_nm_np1_fr_goo_2 : ['pullup']\n",
      "/data/test/pullup/20_Marine_Corps_Pull_Ups_-_JimmyDShea_pullup_f_cm_np1_ba_bad_4 : ['pullup']\n",
      "/data/test/pullup/Pull-ups_+_Push_ups-_a_time-saving_workout_pullup_u_cm_np1_ri_med_1 : ['pullup']\n",
      "/data/test/pullup/Random_Pull_Up_Exercises_pullup_f_nm_np1_fr_med_3 : ['pullup']\n",
      "/data/test/pullup/girl_pull_ups_fitness_exercise_pullups_pullup_f_cm_np1_fr_med_0 : ['pullup']\n",
      "/data/test/pullup/girl_pull_ups_fitness_exercise_pullups_pullup_f_cm_np1_fr_med_2 : ['pullup']\n",
      "/data/test/pullup/30_(dead-hang)_pull-ups_pullup_u_cm_np1_ba_med_2 : ['pullup']\n",
      "/data/test/pullup/20_Marine_Corps_Pull_Ups_-_JimmyDShea_pullup_f_cm_np1_ba_bad_3 : ['pullup']\n",
      "/data/test/pullup/Pull-ups_+_Push_ups-_a_time-saving_workout_pullup_u_cm_np1_ri_med_0 : ['pullup']\n",
      "/data/test/pullup/20_Marine_Corps_Pull_Ups_-_JimmyDShea_pullup_u_cm_np1_ba_bad_2 : ['pullup']\n",
      "/data/test/pullup/Pull-ups_+_Push_ups-_a_time-saving_workout_pullup_u_cm_np1_ri_med_2 : ['pullup']\n",
      "/data/test/pullup/20_Marine_Corps_Pull_Ups_-_JimmyDShea_pullup_u_cm_np1_ba_bad_1 : ['pullup']\n",
      "/data/test/pullup/FREESTYLIN_ON_THE_PULL-BAR_pullup_u_cm_np1_fr_med_0 : ['pullup']\n",
      "/data/test/pullup/Random_Pull_Up_Exercises_pullup_f_nm_np1_fr_med_2 : ['pullup']\n",
      "/data/test/pullup/30_(dead-hang)_pull-ups_pullup_u_cm_np1_ba_med_0 : ['pullup']\n",
      "/data/test/pullup/Pull-ups-_Nicole_Weeks_pullup_u_cm_np1_le_goo_0 : ['pullup']\n",
      "/data/test/pullup/Pull-ups-_Nicole_Weeks_pullup_u_cm_np1_le_goo_1 : ['pullup']\n",
      "/data/test/pullup/35_pull_ups_pullup_f_nm_np1_fr_goo_0 : ['pullup']\n",
      "/data/test/pullup/32_Pull-ups_at_193IBS_pullup_f_cm_np1_ba_med_0 : ['pullup']\n",
      "/data/test/pullup/girl_pull_ups_fitness_exercise_pullups_pullup_f_cm_np1_fr_med_1 : ['pullup']\n",
      "/data/test/pullup/35_pull_ups_pullup_f_nm_np1_fr_goo_1 : ['pullup']\n",
      "/data/test/pullup/30_(dead-hang)_pull-ups_pullup_u_cm_np1_ba_med_1 : ['pullup']\n",
      "/data/test/pullup/Random_Pull_Up_Exercises_pullup_f_nm_np1_ba_med_1 : ['pullup']\n",
      "/data/test/pullup/32_Pull-ups_at_193IBS_pullup_f_cm_np1_ba_med_2 : ['pullup']\n",
      "/data/test/pullup/Random_Pull_Up_Exercises_pullup_f_nm_np1_ba_med_0 : ['pullup']\n",
      "/data/test/pullup/Pull-ups-_Nicole_Weeks_pullup_u_cm_np1_le_goo_2 : ['pullup']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 06:50:14,595 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\r\n"
     ]
    }
   ],
   "source": [
    "!tao action_recognition inference \\\n",
    "                    -e $SPECS_DIR/infer_rgb.yaml \\\n",
    "                    -k $KEY \\\n",
    "                    model=$RESULTS_DIR/rgb_3d_ptm/rgb_only_model.tlt \\\n",
    "                    inference_dataset_dir=$DATA_DIR/test/pullup \\\n",
    "                    video_inf_mode=center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy! <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $HOST_RESULTS_DIR/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 06:55:17,486 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-01 06:55:17,647 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2022-06-01 06:55:17,725 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/sandia/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2022-05-31 21:55:23 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/scripts/export.py:155: UserWarning: \n",
      "    'export_rgb.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "ResNet3d(\n",
      "  (conv1): Conv3d(3, 64, kernel_size=(5, 7, 7), stride=(2, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=2, padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc_cls): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "[NeMo W 2022-05-31 21:55:25 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "      stream(template_mgs % msg_args)\n",
      "    \n",
      "graph(%input_rgb : Float(*, 3, 3, 224, 224, strides=[451584, 150528, 50176, 224, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc_cls.weight : Float(3, 512, strides=[512, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc_cls.bias : Float(3, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %200 : Float(64, 3, 5, 7, 7, strides=[735, 245, 49, 7, 1], requires_grad=0, device=cuda:0),\n",
      "      %201 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %203 : Float(64, 64, 3, 3, 3, strides=[1728, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %204 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %206 : Float(64, 64, 3, 3, 3, strides=[1728, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %207 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %209 : Float(64, 64, 3, 3, 3, strides=[1728, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %210 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %212 : Float(64, 64, 3, 3, 3, strides=[1728, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %213 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %215 : Float(128, 64, 3, 3, 3, strides=[1728, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %216 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %218 : Float(128, 128, 3, 3, 3, strides=[3456, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %219 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %221 : Float(128, 64, 1, 1, 1, strides=[64, 1, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %222 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %224 : Float(128, 128, 3, 3, 3, strides=[3456, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %225 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %227 : Float(128, 128, 3, 3, 3, strides=[3456, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %228 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %230 : Float(256, 128, 3, 3, 3, strides=[3456, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %231 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %233 : Float(256, 256, 3, 3, 3, strides=[6912, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %234 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %236 : Float(256, 128, 1, 1, 1, strides=[128, 1, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %237 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %239 : Float(256, 256, 3, 3, 3, strides=[6912, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %240 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %242 : Float(256, 256, 3, 3, 3, strides=[6912, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %243 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %245 : Float(512, 256, 3, 3, 3, strides=[6912, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %246 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %248 : Float(512, 512, 3, 3, 3, strides=[13824, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %249 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %251 : Float(512, 256, 1, 1, 1, strides=[256, 1, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %252 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %254 : Float(512, 512, 3, 3, 3, strides=[13824, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %255 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %257 : Float(512, 512, 3, 3, 3, strides=[13824, 27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %258 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %259 : Long(1, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %199 : Float(*, 64, 2, 112, 112, strides=[1605632, 25088, 12544, 112, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[5, 7, 7], pads=[2, 3, 3, 2, 3, 3], strides=[2, 2, 2]](%input_rgb, %200, %201)\n",
      "  %125 : Float(*, 64, 2, 112, 112, strides=[1605632, 25088, 12544, 112, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%199) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %126 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[kernel_shape=[1, 3, 3], pads=[0, 1, 1, 0, 1, 1], strides=[2, 2, 2]](%125)\n",
      "  %202 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%126, %203, %204)\n",
      "  %129 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%202) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %205 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%129, %206, %207)\n",
      "  %132 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Add(%205, %126) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %133 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%132)\n",
      "  %208 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%133, %209, %210)\n",
      "  %136 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%208) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %211 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%136, %212, %213)\n",
      "  %139 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Add(%211, %133) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %140 : Float(*, 64, 1, 56, 56, strides=[200704, 3136, 3136, 56, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%139) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %214 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 2, 2]](%140, %215, %216)\n",
      "  %143 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%214) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %217 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%143, %218, %219)\n",
      "  %220 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[1, 1, 1], pads=[0, 0, 0, 0, 0, 0], strides=[1, 2, 2]](%140, %221, %222)\n",
      "  %148 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Add(%217, %220) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %149 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%148)\n",
      "  %223 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%149, %224, %225)\n",
      "  %152 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%223) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %226 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%152, %227, %228)\n",
      "  %155 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Add(%226, %149) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %156 : Float(*, 128, 1, 28, 28, strides=[100352, 784, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%155) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %229 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 2, 2]](%156, %230, %231)\n",
      "  %159 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%229) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %232 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%159, %233, %234)\n",
      "  %235 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[1, 1, 1], pads=[0, 0, 0, 0, 0, 0], strides=[1, 2, 2]](%156, %236, %237)\n",
      "  %164 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Add(%232, %235) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %165 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%164)\n",
      "  %238 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%165, %239, %240)\n",
      "  %168 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%238) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %241 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%168, %242, %243)\n",
      "  %171 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Add(%241, %165) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %172 : Float(*, 256, 1, 14, 14, strides=[50176, 196, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%171) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %244 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 2, 2]](%172, %245, %246)\n",
      "  %175 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%244) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %247 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%175, %248, %249)\n",
      "  %250 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[1, 1, 1], pads=[0, 0, 0, 0, 0, 0], strides=[1, 2, 2]](%172, %251, %252)\n",
      "  %180 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Add(%247, %250) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %181 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%180)\n",
      "  %253 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%181, %254, %255)\n",
      "  %184 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%253) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %256 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%184, %257, %258)\n",
      "  %187 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Add(%256, %181) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:70:0\n",
      "  %188 : Float(*, 512, 1, 7, 7, strides=[25088, 49, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%187) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1296:0\n",
      "  %189 : Float(*, 512, 1, 1, 1, strides=[512, 1, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%188) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1147:0\n",
      "  %190 : Long(5, strides=[1], device=cpu) = onnx::Shape(%189)\n",
      "  %191 : Long(device=cpu) = onnx::Constant[value={0}]()\n",
      "  %192 : Long(device=cpu) = onnx::Gather[axis=0](%190, %191) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:246:0\n",
      "  %194 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%192)\n",
      "  %196 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0](%194, %259)\n",
      "  %197 : Float(*, *, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Reshape(%189, %196) # /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/cv/action_recognition/model/resnet3d.py:246:0\n",
      "  %fc_pred : Float(*, 3, strides=[3, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%197, %fc_cls.weight, %fc_cls.bias) # /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1847:0\n",
      "  return (%fc_pred)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-01 06:55:34,107 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\r\n"
     ]
    }
   ],
   "source": [
    "# Export the RGB model to encrypted ONNX model\n",
    "!tao action_recognition export \\\n",
    "                   -e $SPECS_DIR/export_rgb.yaml \\\n",
    "                   -k $KEY \\\n",
    "                   model=$RESULTS_DIR/rgb_3d_ptm/rgb_only_model.tlt\\\n",
    "                   output_file=$RESULTS_DIR/export/rgb_resnet18_3.etlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model:\n",
      "------------\n",
      "total 127M\r\n",
      "-rw-r--r-- 1 root root 127M  6월  1 06:55 rgb_resnet18_3.etlt\r\n"
     ]
    }
   ],
   "source": [
    "print('Exported model:')\n",
    "print('------------')\n",
    "!ls -lth $HOST_RESULTS_DIR/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end. You may continue by deploying this model to [DeepStream](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_3D_Action.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tao_launch",
   "language": "python",
   "name": "tao_launch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
